import requests
import pandas as pd
from bs4 import BeautifulSoup
import re
import time
import random
from collections import Counter
import urllib3
import warnings

# å¿½ç•¥ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
warnings.filterwarnings('ignore')


class Job1111Crawler:
    """1111 äººåŠ›éŠ€è¡Œè·ç¼ºçˆ¬èŸ²é¡åˆ¥"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬èŸ²"""
        self.base_url = "https://www.1111.com.tw"
        self.setup_session()
        print("ğŸš€ 1111 äººåŠ›éŠ€è¡Œçˆ¬èŸ²å·²åˆå§‹åŒ–")
    
    def setup_session(self):
        """è¨­å®š HTTP session å’Œè«‹æ±‚æ¨™é ­"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.1111.com.tw/',
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.verify = False
    
    def search_jobs(self, keyword="è³‡æ–™å·¥ç¨‹å¸«", page=1, delay=True):
        """
        æœå°‹è·ç¼º
        
        Args:
            keyword (str): æœå°‹é—œéµå­—
            page (int): é æ•¸
            delay (bool): æ˜¯å¦åŠ å…¥éš¨æ©Ÿå»¶é²
        
        Returns:
            str: HTML å…§å®¹ï¼Œå¤±æ•—æ™‚è¿”å› None
        """
        params = {
            'ks': keyword,
            'page': page,
            'order': 1  # 1=æœ€æ–°, 2=ç›¸é—œåº¦
        }
        
        try:
            if delay:
                # éš¨æ©Ÿå»¶é²é¿å…è¢«å°é–
                time.sleep(random.uniform(1, 3))
            
            print(f"ğŸ” æœå°‹é—œéµå­—: {keyword}, ç¬¬ {page} é ")
            
            response = self.session.get(
                f"{self.base_url}/search/job", 
                params=params, 
                timeout=15
            )
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                print(f"âœ… æˆåŠŸç²å–æœå°‹çµæœ")
                return response.text
            else:
                print(f"âŒ æœå°‹å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
                return None
                
        except requests.exceptions.Timeout:
            print("â° è«‹æ±‚è¶…æ™‚")
            return None
        except requests.exceptions.ConnectionError:
            print("ğŸŒ é€£ç·šéŒ¯èª¤")
            return None
        except Exception as e:
            print(f"âŒ æœå°‹éŒ¯èª¤: {e}")
            return None
    
    def parse_jobs(self, html_content):
        """
        è§£æè·ç¼ºè³‡è¨Š
        
        Args:
            html_content (str): HTML å…§å®¹
        
        Returns:
            list: è·ç¼ºè³‡è¨Šåˆ—è¡¨
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        jobs = []
        
        # ä½¿ç”¨ .job-card é¸æ“‡å™¨æ‰¾åˆ°è·ç¼ºå¡ç‰‡
        job_cards = soup.select('.job-card')
        print(f"ğŸ“‹ æ‰¾åˆ° {len(job_cards)} å€‹è·ç¼ºå¡ç‰‡")
        
        for i, card in enumerate(job_cards):
            job_info = self.extract_job_from_card(card, i+1)
            if job_info:
                jobs.append(job_info)
        
        return jobs
    
    def extract_job_from_card(self, card, index):
        """
        å¾è·ç¼ºå¡ç‰‡æå–è³‡è¨Š
        
        Args:
            card: BeautifulSoup è·ç¼ºå¡ç‰‡å…ƒç´ 
            index (int): è·ç¼ºç·¨è™Ÿ
        
        Returns:
            dict: è·ç¼ºè³‡è¨Šå­—å…¸
        """
        job_info = {'index': index}
        
        try:
            # è·ç¼ºæ¨™é¡Œå’Œé€£çµ
            title_link = card.find('a', href=re.compile(r'/job/'))
            if title_link:
                job_info['title'] = title_link.get_text(strip=True)
                href = title_link.get('href', '')
                job_info['link'] = href if href.startswith('http') else self.base_url + href
            
            # å…¬å¸åç¨±
            company_elem = card.find(class_=re.compile(r'company', re.I))
            if not company_elem:
                # å˜—è©¦å¾æ‰€æœ‰æ–‡å­—ä¸­å°‹æ‰¾å…¬å¸åç¨±
                all_text = card.get_text()
                company_match = re.search(r'([^ï½œ\\n\\r\\t]+(?:è‚¡ä»½æœ‰é™å…¬å¸|æœ‰é™å…¬å¸|å…¬å¸))', all_text)
                if company_match:
                    job_info['company'] = company_match.group(1).strip()
                else:
                    # æå–å¡ç‰‡ä¸­çš„å…¬å¸è³‡è¨Š
                    company_text = all_text.split('\\n')[0] if '\\n' in all_text else all_text[:50]
                    job_info['company'] = company_text.strip()
            else:
                job_info['company'] = company_elem.get_text(strip=True)
            
            # å·¥ä½œåœ°é»
            location_patterns = [
                'å°åŒ—å¸‚', 'æ–°åŒ—å¸‚', 'æ¡ƒåœ’å¸‚', 'å°ä¸­å¸‚', 'å°å—å¸‚', 'é«˜é›„å¸‚', 
                'æ–°ç«¹å¸‚', 'æ–°ç«¹ç¸£', 'åŸºéš†å¸‚', 'å®œè˜­ç¸£', 'è‹—æ —ç¸£', 'å½°åŒ–ç¸£',
                'å—æŠ•ç¸£', 'é›²æ—ç¸£', 'å˜‰ç¾©å¸‚', 'å˜‰ç¾©ç¸£', 'å±æ±ç¸£', 'èŠ±è“®ç¸£',
                'å°æ±ç¸£', 'æ¾æ¹–ç¸£', 'é‡‘é–€ç¸£', 'é€£æ±Ÿç¸£'
            ]
            
            card_text = card.get_text()
            for location in location_patterns:
                if location in card_text:
                    job_info['location'] = location
                    break
            
            # è–ªè³‡è³‡è¨Š
            salary_elem = card.find(class_=re.compile(r'salary|wage|pay', re.I))
            if salary_elem:
                job_info['salary'] = salary_elem.get_text(strip=True)
            else:
                # å¾æ–‡å­—ä¸­å°‹æ‰¾è–ªè³‡æ¨¡å¼
                salary_patterns = [
                    r'æœˆè–ª\\s*(\\d+[,\\d]*\\s*[-~è‡³]\\s*\\d+[,\\d]*|\\d+[,\\d]*)\\s*å…ƒ',
                    r'å¹´è–ª\\s*(\\d+[,\\d]*\\s*[-~è‡³]\\s*\\d+[,\\d]*|\\d+[,\\d]*)\\s*å…ƒ',
                    r'(\\d+[,\\d]*\\s*[-~è‡³]\\s*\\d+[,\\d]*)\\s*å…ƒ',
                    r'è–ªè³‡\\s*(\\d+[,\\d]*\\s*[-~è‡³]\\s*\\d+[,\\d]*)',
                    r'é¢è­°.*ç¶“å¸¸æ€§è–ªè³‡é”(\\d+[,\\d]*)è¬?å…ƒ'
                ]
                
                for pattern in salary_patterns:
                    salary_match = re.search(pattern, card_text)
                    if salary_match:
                        job_info['salary'] = salary_match.group(0)
                        break
            
            # è·ç¼ºæ¢ä»¶å’Œè¦æ±‚
            conditions = card.select('.job-card-condition__text')
            if conditions:
                condition_texts = [cond.get_text(strip=True) for cond in conditions]
                job_info['conditions'] = ' | '.join(condition_texts)
            else:
                # å¾å¡ç‰‡æ–‡å­—ä¸­æå–æ¢ä»¶è³‡è¨Š
                condition_match = re.search(r'([^|]*\\|[^|]*\\|[^|]*)', card_text)
                if condition_match:
                    job_info['conditions'] = condition_match.group(1).strip()
            
            # ç™¼å¸ƒæ™‚é–“
            time_patterns = [r'\\d+/\\d+', r'\\d+å¤©å‰', r'æ˜¨å¤©', r'ä»Šå¤©', r'\\d+å°æ™‚å‰']
            for pattern in time_patterns:
                time_match = re.search(pattern, card_text)
                if time_match:
                    job_info['publish_date'] = time_match.group(0)
                    break
            
            # è·ç¼ºæ‘˜è¦
            summary_elem = card.find(class_='job-summary')
            if summary_elem:
                job_info['summary'] = summary_elem.get_text(strip=True)[:200]
            else:
                # æå–å¡ç‰‡çš„éƒ¨åˆ†æ–‡å­—ä½œç‚ºæ‘˜è¦
                summary_text = card.get_text()[:200]
                job_info['summary'] = summary_text.replace('\\n', ' ').strip()
            
            # è¨ˆç®—ç›¸é—œåº¦è©•åˆ†
            job_info['relevance_score'] = self.calculate_relevance_score(card_text)
            
            # ç¢ºä¿è‡³å°‘æœ‰æ¨™é¡Œæ‰è¿”å›
            return job_info if job_info.get('title') else None
            
        except Exception as e:
            print(f"âš ï¸ è§£æè·ç¼ºå¡ç‰‡ {index} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def calculate_relevance_score(self, text):
        """
        è¨ˆç®—è·ç¼ºç›¸é—œåº¦è©•åˆ†
        
        Args:
            text (str): è·ç¼ºæ–‡å­—å…§å®¹
        
        Returns:
            int: ç›¸é—œåº¦è©•åˆ†
        """
        text_lower = text.lower()
        
        # è³‡æ–™å·¥ç¨‹ç›¸é—œé—œéµå­—
        data_keywords = [
            'è³‡æ–™', 'data', 'æ•¸æ“š', 'åˆ†æ', 'analytics', 
            'etl', 'sql', 'python', 'spark', 'hadoop',
            'big data', 'å¤§æ•¸æ“š', 'warehouse', 'å€‰å„²',
            'pipeline', 'ç®¡é“', 'kafka', 'airflow',
            'mongodb', 'mysql', 'postgresql', 'redis',
            'aws', 'azure', 'gcp', 'cloud', 'é›²ç«¯'
        ]
        
        score = 0
        for keyword in data_keywords:
            if keyword in text_lower:
                score += 1
        
        return score
    
    def display_jobs(self, jobs):
        """
        ç¾åŒ–é¡¯ç¤ºè·ç¼ºè³‡è¨Š
        
        Args:
            jobs (list): è·ç¼ºè³‡è¨Šåˆ—è¡¨
        """
        if not jobs:
            print("âŒ æ²’æœ‰æ‰¾åˆ°è·ç¼ºè³‡æ–™")
            return
        
        print(f"\\nğŸ¯ æˆåŠŸçˆ¬å– {len(jobs)} å€‹è·ç¼º")
        print("=" * 80)
        
        for job in jobs:
            print(f"\\nğŸ“‹ è·ç¼º {job.get('index', 'N/A')}")
            print(f"ğŸ¢ æ¨™é¡Œ: {job.get('title', 'N/A')}")
            print(f"ğŸª å…¬å¸: {job.get('company', 'N/A')}")
            print(f"ğŸ“ åœ°é»: {job.get('location', 'N/A')}")
            print(f"ğŸ’° è–ªè³‡: {job.get('salary', 'N/A')}")
            print(f"ğŸ“ æ¢ä»¶: {job.get('conditions', 'N/A')}")
            print(f"ğŸ“… ç™¼å¸ƒ: {job.get('publish_date', 'N/A')}")
            print(f"â­ ç›¸é—œåº¦: {job.get('relevance_score', 'N/A')}")
            
            if job.get('summary'):
                summary = job['summary'][:100] + "..." if len(job['summary']) > 100 else job['summary']
                print(f"ğŸ“„ æ‘˜è¦: {summary}")
            
            if job.get('link'):
                print(f"ğŸ”— é€£çµ: {job['link']}")
            
            print("-" * 60)
    
    def save_to_csv(self, jobs, filename='1111_jobs.csv'):
        """
        å„²å­˜è·ç¼ºè³‡æ–™åˆ° CSV æª”æ¡ˆ
        
        Args:
            jobs (list): è·ç¼ºè³‡è¨Šåˆ—è¡¨
            filename (str): æª”æ¡ˆåç¨±
        
        Returns:
            pandas.DataFrame: è·ç¼ºè³‡æ–™æ¡†
        """
        if not jobs:
            print("âŒ æ²’æœ‰è·ç¼ºè³‡æ–™å¯å„²å­˜")
            return None
        
        df = pd.DataFrame(jobs)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… å·²å„²å­˜ {len(jobs)} ç­†è·ç¼ºè³‡æ–™åˆ° {filename}")
        return df
    
    def analyze_jobs(self, jobs):
        """
        åˆ†æè·ç¼ºçµ±è¨ˆè³‡è¨Š
        
        Args:
            jobs (list): è·ç¼ºè³‡è¨Šåˆ—è¡¨
        """
        if not jobs:
            print("âŒ æ²’æœ‰è·ç¼ºè³‡æ–™å¯åˆ†æ")
            return
        
        print(f"\\nğŸ“Š çµ±è¨ˆåˆ†æ")
        print("=" * 50)
        
        # å…¬å¸çµ±è¨ˆ
        companies = [job.get('company') for job in jobs 
                    if job.get('company') and job.get('company') != 'N/A']
        if companies:
            company_counts = Counter(companies)
            print(f"ğŸ“ˆ å…¬å¸åˆ†å¸ƒ (å‰5å):")
            for company, count in company_counts.most_common(5):
                print(f"   {company}: {count} å€‹è·ç¼º")
        
        # åœ°é»çµ±è¨ˆ
        locations = [job.get('location') for job in jobs 
                    if job.get('location') and job.get('location') != 'N/A']
        if locations:
            location_counts = Counter(locations)
            print(f"\\nğŸ“ åœ°é»åˆ†å¸ƒ:")
            for location, count in location_counts.most_common():
                print(f"   {location}: {count} å€‹è·ç¼º")
        
        # ç›¸é—œåº¦çµ±è¨ˆ
        relevance_scores = [job.get('relevance_score', 0) for job in jobs]
        if relevance_scores:
            avg_relevance = sum(relevance_scores) / len(relevance_scores)
            print(f"\\nâ­ å¹³å‡ç›¸é—œåº¦: {avg_relevance:.2f}")
            print(f"   æœ€é«˜ç›¸é—œåº¦: {max(relevance_scores)}")
            print(f"   æœ€ä½ç›¸é—œåº¦: {min(relevance_scores)}")
        
        # è–ªè³‡çµ±è¨ˆ
        salaries = [job.get('salary') for job in jobs 
                   if job.get('salary') and job.get('salary') != 'N/A']
        print(f"\\nğŸ’° æœ‰è–ªè³‡è³‡è¨Šçš„è·ç¼º: {len(salaries)} å€‹ ({len(salaries)/len(jobs)*100:.1f}%)")
        
        # ç™¼å¸ƒæ™‚é–“çµ±è¨ˆ
        publish_dates = [job.get('publish_date') for job in jobs 
                        if job.get('publish_date') and job.get('publish_date') != 'N/A']
        print(f"ğŸ“… æœ‰ç™¼å¸ƒæ™‚é–“çš„è·ç¼º: {len(publish_dates)} å€‹ ({len(publish_dates)/len(jobs)*100:.1f}%)")
    
    def crawl_multiple_pages(self, keyword="è³‡æ–™å·¥ç¨‹å¸«", max_pages=3):
        """
        çˆ¬å–å¤šé è·ç¼ºè³‡æ–™
        
        Args:
            keyword (str): æœå°‹é—œéµå­—
            max_pages (int): æœ€å¤§é æ•¸
        
        Returns:
            list: æ‰€æœ‰è·ç¼ºè³‡è¨Šåˆ—è¡¨
        """
        all_jobs = []
        
        for page in range(1, max_pages + 1):
            print(f"\\nğŸ”„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é ...")
            
            html_content = self.search_jobs(keyword, page)
            if html_content:
                jobs = self.parse_jobs(html_content)
                if jobs:
                    all_jobs.extend(jobs)
                    print(f"âœ… ç¬¬ {page} é çˆ¬å–å®Œæˆï¼Œç²å¾— {len(jobs)} å€‹è·ç¼º")
                else:
                    print(f"âš ï¸ ç¬¬ {page} é æ²’æœ‰æ‰¾åˆ°è·ç¼º")
                    break
            else:
                print(f"âŒ ç¬¬ {page} é çˆ¬å–å¤±æ•—")
                break
            
            # é é¢é–“å»¶é²
            if page < max_pages:
                time.sleep(random.uniform(2, 4))
        
        # é‡æ–°ç·¨è™Ÿ
        for i, job in enumerate(all_jobs):
            job['index'] = i + 1
        
        return all_jobs


def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸš€ 1111 äººåŠ›éŠ€è¡Œè·ç¼ºçˆ¬èŸ²")
    print("=" * 50)
    
    # å»ºç«‹çˆ¬èŸ²å¯¦ä¾‹
    crawler = Job1111Crawler()
    
    # è¨­å®šæœå°‹åƒæ•¸
    keyword = input("è«‹è¼¸å…¥æœå°‹é—œéµå­— (é è¨­: è³‡æ–™å·¥ç¨‹å¸«): ").strip() or "è³‡æ–™å·¥ç¨‹å¸«"
    
    try:
        max_pages = int(input("è«‹è¼¸å…¥è¦çˆ¬å–çš„é æ•¸ (é è¨­: 1): ").strip() or "1")
    except ValueError:
        max_pages = 1
    
    # é–‹å§‹çˆ¬å–
    print(f"\\nğŸ¯ é–‹å§‹æœå°‹ '{keyword}' ç›¸é—œè·ç¼º...")
    
    if max_pages == 1:
        # å–®é çˆ¬å–
        html_content = crawler.search_jobs(keyword)
        if html_content:
            jobs = crawler.parse_jobs(html_content)
        else:
            jobs = []
    else:
        # å¤šé çˆ¬å–
        jobs = crawler.crawl_multiple_pages(keyword, max_pages)
    
    if jobs:
        # é¡¯ç¤ºçµæœ
        crawler.display_jobs(jobs)
        
        # çµ±è¨ˆåˆ†æ
        crawler.analyze_jobs(jobs)
        
        # å„²å­˜æª”æ¡ˆ
        filename = f"/Users/txwu/project/data/GetOffer/data/1111_{keyword.replace(' ', '_')}_jobs.csv"
        df = crawler.save_to_csv(jobs, filename)
        
        print(f"\\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±ç²å¾— {len(jobs)} å€‹è·ç¼ºè³‡æ–™")
        
    else:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•è·ç¼ºè³‡æ–™")


if __name__ == "__main__":
    main()